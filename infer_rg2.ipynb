{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 读文件\n",
    "- 1.休息西\n",
    "### 传入模型出热图\n",
    "### 保存结果 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import h5py as h5\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "cur_path = os.path.abspath(\".\")\n",
    "data_path = os.path.join(cur_path,'data')\n",
    "print(f\"cur_path:{cur_path}\")\n",
    "print(f\"data_path:{data_path}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cur_path:/home/xindong/project/rg2-work\n",
      "data_path:/home/xindong/project/rg2-work/data\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "USE_MY_DETE = False\n",
    "USE_RESAMPLE_DATA = False\n",
    "USE_PAD_DATA=True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def resample_image(itk_image:sitk.Image,target_size =[1536,1536,1], is_label=False):\n",
    "    original_spacing = itk_image.GetSpacing()\n",
    "    original_size = itk_image.GetSize()\n",
    "    min_spacing_id = original_size.index(min(original_size[:-1]))\n",
    "    print(original_size)\n",
    "    print(min_spacing_id)\n",
    "    min_spacing = original_spacing[min_spacing_id]*original_size[min_spacing_id]/target_size[min_spacing_id]\n",
    "    target_spacing = [min_spacing,min_spacing,1]\n",
    "    print(f\"spacing:{target_spacing}\")\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(target_spacing)\n",
    "    resample.SetSize(target_size)\n",
    "    resample.SetOutputDirection(itk_image.GetDirection())\n",
    "    resample.SetOutputOrigin(itk_image.GetOrigin())\n",
    "    # resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    return resample.Execute(itk_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "if USE_MY_DETE:\n",
    "    img = sitk.ReadImage(os.path.join(data_path,'022'))\n",
    "elif USE_RESAMPLE_DATA:\n",
    "    img = sitk.ReadImage(os.path.join(data_path,'022'))\n",
    "    img = resample_image(img)\n",
    "    # img = sitk.ReadImage(os.path.join(data_path,'022.nii.gz'))\n",
    "elif USE_PAD_DATA:\n",
    "    img = sitk.ReadImage(os.path.join(data_path,'022'))\n",
    "else:\n",
    "    img = sitk.ReadImage(os.path.join(data_path,'image006.nii.gz'))\n",
    "\n",
    "print(img.GetSize())\n",
    "xray_data = sitk.GetArrayFromImage(img)\n",
    "\n",
    "xray_data = xray_data.squeeze()\n",
    "\n",
    "if not np.issubdtype(xray_data.dtype,np.float32) :\n",
    "    xray_data = xray_data.astype(np.float32)\n",
    "\n",
    "print(type(xray_data),xray_data.shape,xray_data.dtype)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(978, 1067, 1)\n",
      "<class 'numpy.ndarray'> (1067, 978) float32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(xray_data[:,:],cmap='gray')\n",
    "plt.show"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# General utility functions\n",
    "#\n",
    "# Copyright (C) 2019-2020 Robert Grupp (grupp@jhu.edu)\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program. If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dice import *\n",
    "\n",
    "def get_gaussian_2d_heatmap(num_rows, num_cols, sigma, peak_row=None, peak_col=None):\n",
    "    if peak_row is None:\n",
    "        peak_row = num_rows // 2\n",
    "    if peak_col is None:\n",
    "        peak_col = num_cols // 2\n",
    "    \n",
    "    (Y,X) = torch.meshgrid(torch.arange(0,num_rows), torch.arange(0,num_cols))\n",
    "    \n",
    "    Y = Y.float()\n",
    "    X = X.float()\n",
    "\n",
    "    return torch.exp(((X - peak_col).pow(2) + (Y - peak_row).pow(2)) / (sigma * sigma * -2)) / (2 * math.pi * sigma * sigma)\n",
    "\n",
    "def write_floats_to_txt(file_path, floats):\n",
    "    with open(file_path,'w') as out:\n",
    "        for f in floats:\n",
    "            out.write('{:.6f}\\n'.format(f))\n",
    "        out.flush()\n",
    "\n",
    "def read_floats_from_txt(file_path):\n",
    "    return torch.Tensor([float(l.strip()) for l in open(file_path).readlines()])\n",
    "\n",
    "class RunningFloatWriter:\n",
    "    def __init__(self, file_path, new_file=True):\n",
    "        super(RunningFloatWriter,self).__init__()\n",
    "\n",
    "        write_mode = 'w'\n",
    "        if not new_file:\n",
    "            write_mode = 'a'\n",
    "\n",
    "        self.out = open(file_path, write_mode)\n",
    "\n",
    "    def write(self, x):\n",
    "        self.out.write('{:.6f}\\n'.format(x))\n",
    "        self.out.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.out:\n",
    "            self.out.flush()\n",
    "            self.out.close()\n",
    "            self.out = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def center_crop(img, dst_shape):\n",
    "    src_nr = img.shape[-2]\n",
    "    src_nc = img.shape[-1]\n",
    "\n",
    "    dst_nr = dst_shape[-2]\n",
    "    dst_nc = dst_shape[-1]\n",
    "    \n",
    "    if (dst_nr != src_nr) or (dst_nc != src_nc):\n",
    "        src_start_r = int((src_nr - dst_nr) / 2)\n",
    "        src_end_r   = src_start_r + dst_nr\n",
    "        \n",
    "        src_start_c = int((src_nc - dst_nc) / 2)\n",
    "        src_end_c   = src_start_c + dst_nc\n",
    "        \n",
    "        if img.dim() == 4:\n",
    "            return img[:,:,src_start_r:src_end_r,src_start_c:src_end_c]\n",
    "        elif img.dim() == 3:\n",
    "            return img[:,src_start_r:src_end_r,src_start_c:src_end_c]\n",
    "        else:\n",
    "            assert(img.dim() == 2)\n",
    "            return img[src_start_r:src_end_r,src_start_c:src_end_c]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def test_dataset(ds, net, dev=None, num_lands=0):\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "\n",
    "        losses = torch.zeros(len(ds))\n",
    "        \n",
    "        num_items = 0\n",
    "    \n",
    "        if num_lands > 0:\n",
    "            criterion = DiceAndHeatMapLoss2D(skip_bg=False)\n",
    "        else:\n",
    "            criterion = DiceLoss2D(skip_bg=False)\n",
    "\n",
    "        for (i, data) in enumerate(dl, 0):\n",
    "            (projs, masks, lands, heats) = data\n",
    "            \n",
    "            if dev is not None:\n",
    "                projs = projs.to(dev)\n",
    "                masks = masks.to(dev)\n",
    "                if num_lands > 0:\n",
    "                    if len(heats.shape) > 4:\n",
    "                        assert(len(heats.shape) == 5)\n",
    "                        assert(heats.shape[2] == 1)\n",
    "                        heats = heats.view(heats.shape[0], heats.shape[1], heats.shape[3], heats.shape[4])\n",
    "                    heats = heats.to(dev)\n",
    "\n",
    "            net_out = net(projs)\n",
    "            if (num_lands > 0) or (type(net_out) is tuple):\n",
    "                pred_masks     = net_out[0]\n",
    "                pred_heat_maps = net_out[1]\n",
    "            else:\n",
    "                pred_masks = net_out\n",
    "\n",
    "            pred_masks = center_crop(pred_masks, masks.shape)\n",
    "            \n",
    "            if num_lands > 0:\n",
    "                pred_heat_maps = center_crop(pred_heat_maps, heats.shape)\n",
    "                loss = criterion((pred_masks, pred_heat_maps), (masks, heats))\n",
    "            else:\n",
    "                loss = criterion(pred_masks, masks)\n",
    "\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "            num_items += 1\n",
    "        \n",
    "        assert(num_items == len(ds))\n",
    "\n",
    "        return (torch.mean(losses), torch.std(losses))\n",
    "\n",
    "def test_dataset_ensemble(ds, nets, dev=None, num_lands=0, dice_only=False):\n",
    "    num_nets = len(nets)\n",
    "\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for net in nets:\n",
    "            net.eval()\n",
    "\n",
    "        losses = torch.zeros(len(ds))\n",
    "        \n",
    "        num_items = 0\n",
    "    \n",
    "        if not dice_only and (num_lands > 0):\n",
    "            criterion = DiceAndHeatMapLoss2D(skip_bg=False)\n",
    "        else:\n",
    "            criterion = DiceLoss2D(skip_bg=False)\n",
    "\n",
    "        for (i, data) in enumerate(dl, 0):\n",
    "            (projs, masks, lands, heats) = data\n",
    "            \n",
    "            if dev is not None:\n",
    "                projs = projs.to(dev)\n",
    "                masks = masks.to(dev)\n",
    "                if num_lands > 0:\n",
    "                    if len(heats.shape) > 4:\n",
    "                        assert(len(heats.shape) == 5)\n",
    "                        assert(heats.shape[2] == 1)\n",
    "                        heats = heats.view(heats.shape[0], heats.shape[1], heats.shape[3], heats.shape[4])\n",
    "                    heats = heats.to(dev)\n",
    "\n",
    "            avg_masks = None\n",
    "            avg_heats = None\n",
    "\n",
    "            for net in nets:\n",
    "                net_out = net(projs)\n",
    "                if (num_lands > 0) or (type(net_out) is tuple):\n",
    "                    pred_masks     = net_out[0]\n",
    "                    pred_heat_maps = net_out[1]\n",
    "                else:\n",
    "                    pred_masks = net_out\n",
    "\n",
    "                pred_masks = center_crop(pred_masks, masks.shape)\n",
    "\n",
    "                if avg_masks is None:\n",
    "                    avg_masks = pred_masks\n",
    "                else:\n",
    "                    avg_masks += pred_masks\n",
    "            \n",
    "                if num_lands > 0:\n",
    "                    pred_heat_maps = center_crop(pred_heat_maps, heats.shape)\n",
    "\n",
    "                    if avg_heats is None:\n",
    "                        avg_heats = pred_heat_maps\n",
    "                    else:\n",
    "                        avg_heats += pred_heat_maps\n",
    "            # end for net\n",
    "            \n",
    "            avg_masks /= num_nets\n",
    "\n",
    "            if num_lands > 0:\n",
    "                avg_heats /= num_nets\n",
    "\n",
    "            if not dice_only and (num_lands > 0):\n",
    "                loss = criterion((avg_masks, avg_heats), (masks, heats))\n",
    "            else:\n",
    "                loss = criterion(avg_masks, masks)\n",
    "\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "            num_items += 1\n",
    "        \n",
    "        assert(num_items == len(ds))\n",
    "\n",
    "        return (torch.mean(losses), torch.std(losses))\n",
    "\n",
    "def seg_dataset(ds, net, h5_f, dev=None, num_lands=0):\n",
    "    orig_img_shape = ds.rob_orig_img_shape\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "   \n",
    "    dst_ds = h5_f.create_dataset('nn-segs', (len(ds), *orig_img_shape),\n",
    "                                 dtype='u1',\n",
    "                                 chunks=(1, *orig_img_shape),\n",
    "                                 compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    dst_heats_ds = None\n",
    "\n",
    "    if num_lands > 0:\n",
    "        dst_heats_ds = h5_f.create_dataset('nn-heats', (len(ds), num_lands, *orig_img_shape),\n",
    "                                           chunks=(1,1,*orig_img_shape),\n",
    "                                           compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "\n",
    "        num_items = 0\n",
    "        \n",
    "        for (i, data) in enumerate(dl, 0):\n",
    "            projs = data[0]\n",
    "\n",
    "            if dev is not None:\n",
    "                projs = projs.to(dev)\n",
    "\n",
    "            net_out = net(projs)\n",
    "            if (num_lands > 0) or (type(net_out) is tuple):\n",
    "                pred_masks = net_out[0]\n",
    "                pred_heats = net_out[1]\n",
    "            else:\n",
    "                pred_masks = net_out\n",
    "\n",
    "            pred_masks = center_crop(pred_masks, orig_img_shape)\n",
    "\n",
    "            (_, pred_masks) = torch.max(pred_masks, dim=1)\n",
    "   \n",
    "            # write to file\n",
    "            dst_ds[i,:,:] = pred_masks.view(orig_img_shape).cpu().numpy()\n",
    "\n",
    "            if dst_heats_ds is not None:\n",
    "                dst_heats_ds[i,:,:,:] = center_crop(pred_heats, orig_img_shape).numpy()\n",
    "\n",
    "            num_items += 1\n",
    "        \n",
    "        assert(num_items == len(ds))\n",
    "\n",
    "\n",
    "def seg_dataset_ensemble(ds, nets, h5_f, dev=None, num_lands=0, times=None):\n",
    "    num_nets = len(nets)\n",
    "\n",
    "    orig_img_shape = ds.rob_orig_img_shape\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "   \n",
    "    dst_ds = h5_f.create_dataset('nn-segs', (len(ds), *orig_img_shape),\n",
    "                                 dtype='u1',\n",
    "                                 chunks=(1, *orig_img_shape),\n",
    "                                 compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    dst_heats_ds = None\n",
    "\n",
    "    if num_lands > 0:\n",
    "        dst_heats_ds = h5_f.create_dataset('nn-heats', (len(ds), num_lands, *orig_img_shape),\n",
    "                                           chunks=(1,1,*orig_img_shape),\n",
    "                                           compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for net in nets:\n",
    "            net.eval()\n",
    "\n",
    "        num_items = 0\n",
    "        \n",
    "        for (i, data) in enumerate(dl, 0):\n",
    "            projs = data[0]\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            if dev is not None:\n",
    "                projs = projs.to(dev)\n",
    "\n",
    "            avg_masks = None\n",
    "            \n",
    "            avg_heats = None\n",
    "\n",
    "            for net in nets:\n",
    "                net_out = net(projs)\n",
    "                if (num_lands > 0) or (type(net_out) is tuple):\n",
    "                    pred_masks = net_out[0]\n",
    "                    pred_heats = net_out[1]\n",
    "                else:\n",
    "                    pred_masks = net_out\n",
    "\n",
    "                pred_masks = center_crop(pred_masks, orig_img_shape)\n",
    "\n",
    "                if avg_masks is None:\n",
    "                    avg_masks = pred_masks\n",
    "                else:\n",
    "                    avg_masks += pred_masks\n",
    "            \n",
    "                if dst_heats_ds is not None:\n",
    "                    pred_heats = center_crop(pred_heats, orig_img_shape)\n",
    "                    \n",
    "                    pred_heats_min = pred_heats.min().item()\n",
    "                    pred_heats_max = pred_heats.max().item()\n",
    "                    \n",
    "                    pred_heats = (pred_heats - pred_heats_min) / (pred_heats_max - pred_heats_min)\n",
    "\n",
    "                    if avg_heats is None:\n",
    "                        avg_heats = pred_heats\n",
    "                    else:\n",
    "                        avg_heats += pred_heats\n",
    "            \n",
    "            # technically we don't need to do this for the segmentation\n",
    "            avg_masks /= num_nets\n",
    "\n",
    "            (_, pred_masks) = torch.max(avg_masks, dim=1)\n",
    "            \n",
    "            stop_time = time.time()\n",
    "\n",
    "            if times is not None:\n",
    "                times.append(stop_time - start_time)\n",
    "\n",
    "            # write to file\n",
    "            dst_ds[i,:,:] = pred_masks.view(orig_img_shape).cpu().numpy()\n",
    "            \n",
    "            if dst_heats_ds is not None:\n",
    "                avg_heats /= num_nets\n",
    "                dst_heats_ds[i,:,:,:] = avg_heats.cpu().numpy()\n",
    "\n",
    "            num_items += 1\n",
    "        \n",
    "        assert(num_items == len(ds))\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Dataloading utilities from preprocessed HDF5 files.\n",
    "#\n",
    "# Copyright (C) 2019-2020 Robert Grupp (grupp@jhu.edu)\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program. If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import h5py as h5\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "\n",
    "# from util import *\n",
    "            \n",
    "def calc_pad_amount(padded_img_dim, cur_img_dim):\n",
    "    # new pad dimension should be larger\n",
    "    assert(padded_img_dim > cur_img_dim)\n",
    "\n",
    "    # first calculate the amount to pad along the borders\n",
    "    pad = (padded_img_dim - cur_img_dim)/ 2\n",
    "\n",
    "    # handle odd sized input\n",
    "    if pad != int(pad):\n",
    "        pad = int(pad) + 1\n",
    "    else:\n",
    "        # needs to be integral\n",
    "        pad = int(pad)\n",
    "\n",
    "    return pad\n",
    "\n",
    "class RandomDataAugDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, projs, segs, lands=None, proj_pad_dim=0):\n",
    "        self.projs = projs\n",
    "        self.segs  = segs\n",
    "        self.lands = lands\n",
    "   \n",
    "        assert(len(projs.shape) == 4)\n",
    "        assert(projs.shape[1] == 1)\n",
    "\n",
    "        if segs is not None:\n",
    "            assert(len(projs.shape) == len(segs.shape))\n",
    "            assert(projs.shape[0] == segs.shape[0])\n",
    "            \n",
    "            # initial sizes before padding should be equal\n",
    "            assert(projs.shape[2] == segs.shape[2])\n",
    "            assert(projs.shape[3] == segs.shape[3])\n",
    "        \n",
    "        if lands is not None:\n",
    "            assert(projs.shape[0] == lands.shape[0])\n",
    "            assert(lands.shape[1] == 2)\n",
    "\n",
    "        self.prob_of_aug = 0.5\n",
    "        #self.prob_of_aug = 1.0\n",
    "\n",
    "        self.do_invert = True\n",
    "        self.do_gamma  = True\n",
    "        self.do_noise  = True\n",
    "        self.do_affine = True\n",
    "        self.do_erase  = True\n",
    "\n",
    "        self.erase_prob = 0.25\n",
    "\n",
    "        self.pad_data_for_affine = True\n",
    "\n",
    "        self.do_norm_01_scale = True\n",
    "\n",
    "        self.include_heat_map = True\n",
    "\n",
    "        self.print_aug_info = False\n",
    "\n",
    "        self.extra_pad = 0\n",
    "        if proj_pad_dim > 0:\n",
    "            # only support square images for now\n",
    "            assert(projs.shape[-1] == projs.shape[-2])\n",
    "            self.extra_pad = calc_pad_amount(proj_pad_dim, projs.shape[-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.projs.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        assert(type(i) is int)\n",
    "\n",
    "        p = self.projs[i,:,:,:]\n",
    "\n",
    "        s = None\n",
    "        if self.segs is not None:\n",
    "            s = self.segs[i,:,:,:]\n",
    "\n",
    "        cur_lands = None\n",
    "        if self.lands is not None:\n",
    "            # we need a deep copy here because of possible data aug\n",
    "            cur_lands = self.lands[i,:,:].clone()\n",
    "\n",
    "        need_to_pad_proj = self.extra_pad > 0\n",
    "\n",
    "        if (self.prob_of_aug > 0) and (random.random() < self.prob_of_aug): \n",
    "            #print('augmenting...')\n",
    "\n",
    "            if self.do_invert and (random.random() < 0.5):\n",
    "                #print('  inversion...')\n",
    "\n",
    "                p_max = p.max()\n",
    "                #p_min = p.min()\n",
    "                p = p_max - p\n",
    "\n",
    "                if self.print_aug_info:\n",
    "                    print('inverting')\n",
    "\n",
    "            if self.do_noise:\n",
    "                # normalize to [0,1] to apply noise\n",
    "                p_min = p.min()\n",
    "                p_max = p.max()\n",
    "\n",
    "                p = (p - p_min) / (p_max - p_min)\n",
    "\n",
    "                cur_noise_sigma = random.uniform(0.005, 0.01)\n",
    "                p += torch.randn(p.shape) * cur_noise_sigma\n",
    "                \n",
    "                p = (p * (p_max - p_min)) + p_min\n",
    "\n",
    "                if self.print_aug_info:\n",
    "                    print('noise sigma: {:.3f}'.format(cur_noise_sigma))\n",
    "\n",
    "            if self.do_gamma:\n",
    "                # normalize to [0,1] to apply gamma\n",
    "                p_min = p.min()\n",
    "                p_max = p.max()\n",
    "\n",
    "                p = (p - p_min) / (p_max - p_min)\n",
    "\n",
    "                gamma = random.uniform(0.7,1.3)\n",
    "                p.pow_(gamma)\n",
    "\n",
    "                p = (p * (p_max - p_min)) + p_min\n",
    "\n",
    "                if self.print_aug_info:\n",
    "                    print('gamma = {:.2f}'.format(gamma))\n",
    "       \n",
    "            if self.do_affine:\n",
    "                # data needs to be in [0,1] for PIL functions\n",
    "                p_min = p.min()\n",
    "                p_max = p.max()\n",
    "\n",
    "                p = (p - p_min) / (p_max - p_min)\n",
    "                \n",
    "                orig_p_shape = p.shape\n",
    "                if self.pad_data_for_affine:\n",
    "                    pad1 = int(math.ceil(orig_p_shape[1] / 2.0))\n",
    "                    pad2 = int(math.ceil(orig_p_shape[2] / 2.0))\n",
    "                    if need_to_pad_proj:\n",
    "                        pad1 += self.extra_pad\n",
    "                        pad2 += self.extra_pad\n",
    "                        need_to_pad_proj = False\n",
    "\n",
    "                    p = torch.from_numpy(np.pad(p.numpy(),\n",
    "                                                ((0,0), (pad1,pad1), (pad2,pad2)),\n",
    "                                                'reflect'))\n",
    "                \n",
    "                p_il = TF.to_pil_image(p)\n",
    "\n",
    "                # this uniformly samples the direction\n",
    "                rand_trans = torch.randn(2)\n",
    "                rand_trans /= rand_trans.norm()\n",
    "\n",
    "                # now uniformly sample the magnitdue\n",
    "                rand_trans *= random.random() * 20\n",
    "                \n",
    "                rot_ang = random.uniform(-5, 5)\n",
    "                trans_x = rand_trans[0]\n",
    "                trans_y = rand_trans[1]\n",
    "                shear_x   = random.uniform(-2, 2)\n",
    "                shear_y   = random.uniform(-2, 2)\n",
    "                shear   = [shear_x,shear_x]\n",
    "                \n",
    "                scale_factor = random.uniform(0.9, 1.1)\n",
    "\n",
    "                if self.print_aug_info:\n",
    "                    print('Rot: {:.2f}'.format(rot_ang))\n",
    "                    print('Trans X: {:.2f} , Trans Y: {:.2f}'.format(trans_x, trans_y))\n",
    "                    print('Shear X: {:.2f},  Shaer Y: {:.2f}'.format(shear_x,shear_y))\n",
    "                    print('Scale: {:.2f}'.format(scale_factor))\n",
    "\n",
    "                p = TF.to_tensor(TF.affine(TF.to_pil_image(p),\n",
    "                                 rot_ang,\n",
    "                                 (trans_x, trans_y),\n",
    "                                 scale_factor,\n",
    "                                 shear,\n",
    "                                 resample=PIL.Image.BILINEAR))\n",
    "                \n",
    "                if self.pad_data_for_affine:\n",
    "                    # pad can be zero\n",
    "                    pad_shape = (orig_p_shape[-2] + (2 * self.extra_pad), orig_p_shape[-1] + (2 * self.extra_pad))\n",
    "                    p = center_crop(p, pad_shape)\n",
    "\n",
    "                p = (p * (p_max - p_min)) + p_min\n",
    "\n",
    "                if s is not None:\n",
    "                    orig_s_shape = s.shape\n",
    "                    if self.pad_data_for_affine:\n",
    "                        pad1 = int(math.ceil(orig_s_shape[1] / 2.0))\n",
    "                        pad2 = int(math.ceil(orig_s_shape[2] / 2.0))\n",
    "                        s = torch.from_numpy(np.pad(s.numpy(),\n",
    "                                                    ((0,0), (pad1,pad1), (pad2,pad2)),\n",
    "                                                    'reflect'))\n",
    "                    \n",
    "                    # warp each class separately, I don't want any wacky color\n",
    "                    # spaces assumed by PIL\n",
    "                    for c in range(s.shape[0]):\n",
    "                        s[c,:,:] = TF.to_tensor(TF.affine(TF.to_pil_image(s[c,:,:]),\n",
    "                                                          rot_ang,\n",
    "                                                          (trans_x, trans_y),\n",
    "                                                          scale_factor,\n",
    "                                                          shear))\n",
    "                    if self.pad_data_for_affine:\n",
    "                        s = center_crop(s, orig_s_shape)\n",
    "                \n",
    "                if cur_lands is not None:\n",
    "                    shape_for_center_of_rot = s.shape if s is not None else p.shape\n",
    "\n",
    "                    center_of_rot = ((shape_for_center_of_rot[-2] / 2.0) + 0.5,\n",
    "                                     (shape_for_center_of_rot[-1] / 2.0) + 0.5)\n",
    "                    \n",
    "                    A_inv = TF._get_inverse_affine_matrix(center_of_rot, rot_ang, (trans_x, trans_y), scale_factor, shear)\n",
    "                    A = np.matrix([ [A_inv[0], A_inv[1], A_inv[2]], [A_inv[3], A_inv[4], A_inv[5]], [0,0,1]]).I\n",
    "\n",
    "                    for pt_idx in range(cur_lands.shape[-1]):\n",
    "                        cur_land = cur_lands[:,pt_idx]\n",
    "                        if (not math.isinf(cur_land[0])) and (not math.isinf(cur_land[1])):\n",
    "                            tmp_pt = A * np.asmatrix(np.pad(cur_land.numpy(), (0,1), mode='constant', constant_values=1).reshape(3,1))\n",
    "                            xform_l = torch.from_numpy(np.squeeze(np.asarray(tmp_pt))[0:2])\n",
    "                            if (s is not None) and \\\n",
    "                               ((xform_l[0] < 0) or (xform_l[0] > (orig_s_shape[1] - 1)) or \\\n",
    "                                (xform_l[1] < 0) or (xform_l[1] < (orig_s_shape[0] - 1))):\n",
    "                                xform_l[0] = math.inf\n",
    "                                xform_l[1] = math.inf\n",
    "                            \n",
    "                            cur_lands[:,pt_idx] = xform_l\n",
    "            \n",
    "            if self.do_erase and (random.random() < self.erase_prob):\n",
    "                #print('  box noise/erase...')\n",
    "\n",
    "                p_2d_shape = [p.shape[-2], p.shape[-1]]\n",
    "                box_mean_dim = torch.Tensor([p_2d_shape[0] * 0.15, p_2d_shape[1] * 0.15])\n",
    "                \n",
    "                num_boxes = random.randint(1,5)\n",
    "                \n",
    "                if self.print_aug_info:\n",
    "                    print('  Random Corrupt: num. boxes: {}'.format(num_boxes))\n",
    "                \n",
    "                for box_idx in range(num_boxes):\n",
    "                    box_valid = False\n",
    "                    \n",
    "                    while not box_valid:\n",
    "                        # First sample box dims\n",
    "                        box_dims = torch.round((torch.randn(2) * (box_mean_dim)) + box_mean_dim).long()\n",
    "\n",
    "                        if (box_dims[0] > 0) and (box_dims[1] > 0) and \\\n",
    "                                (box_dims[0] <= p_2d_shape[0]) and (box_dims[1] <= p_2d_shape[1]):\n",
    "                            # Next sample box location\n",
    "                            start_row = random.randint(0, p_2d_shape[0] - box_dims[0])\n",
    "                            start_col = random.randint(0, p_2d_shape[1] - box_dims[1])\n",
    "\n",
    "                            box_valid = True\n",
    "                    \n",
    "                    p_roi = p[0,start_row:(start_row+box_dims[0]),start_col:(start_col+box_dims[1])]\n",
    "\n",
    "                    sigma_noise = (p_roi.max() - p_roi.min()) * 0.2\n",
    "                    \n",
    "                    p_roi += torch.randn(p_roi.shape) * sigma_noise\n",
    "\n",
    "        # end data aug\n",
    "\n",
    "        if need_to_pad_proj:\n",
    "            p = torch.from_numpy(np.pad(p.numpy(),\n",
    "                                 ((0, 0), (self.extra_pad, self.extra_pad), (self.extra_pad, self.extra_pad)),\n",
    "                                 'reflect'))\n",
    "\n",
    "        if self.do_norm_01_scale:\n",
    "            p = (p - p.mean()) / p.std()\n",
    "\n",
    "        h = None\n",
    "        if self.include_heat_map:\n",
    "            assert(s is not None)\n",
    "            assert(cur_lands is not None)\n",
    "\n",
    "            num_lands = cur_lands.shape[-1]\n",
    "\n",
    "            h = torch.zeros(num_lands, 1, s.shape[-2], s.shape[-1])\n",
    "\n",
    "            # \"FH-l\", \"FH-r\", \"GSN-l\", \"GSN-r\", \"IOF-l\", \"IOF-r\", \"MOF-l\", \"MOF-r\", \"SPS-l\", \"SPS-r\", \"IPS-l\", \"IPS-r\"\n",
    "            #sigma_lut = [ 2.5, 2.5, 7.5, 7.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5]\n",
    "            sigma_lut = torch.full([num_lands], 2.5)\n",
    "\n",
    "            (Y,X) = torch.meshgrid(torch.arange(0, s.shape[-2]),\n",
    "                                   torch.arange(0, s.shape[-1]))\n",
    "            Y = Y.float()\n",
    "            X = X.float()\n",
    "\n",
    "            for land_idx in range(num_lands):\n",
    "                sigma = sigma_lut[land_idx]\n",
    "\n",
    "                cur_land = cur_lands[:,land_idx]\n",
    "\n",
    "                mu_x = cur_land[0]\n",
    "                mu_y = cur_land[1]\n",
    "\n",
    "                if not math.isinf(mu_x) and not math.isinf(mu_y):\n",
    "                    pdf = torch.exp(((X - mu_x).pow(2) + (Y - mu_y).pow(2)) / (sigma * sigma * -2)) / (2 * math.pi * sigma * sigma)\n",
    "                    #pdf /= pdf.sum() # normalize to sum of 1\n",
    "                    h[land_idx,0,:,:] = pdf\n",
    "            #assert(torch.all(torch.isfinite(h)))\n",
    "\n",
    "        return (p,s,cur_lands,h)\n",
    "\n",
    "def get_orig_img_shape(h5_file_path, pat_ind):\n",
    "    f = h5.File(h5_file_path, 'r')\n",
    "        \n",
    "    s = f['{:02d}/projs'.format(pat_ind)].shape\n",
    "    \n",
    "    assert(len(s) == 3)\n",
    "    \n",
    "    return (s[1], s[2])\n",
    "\n",
    "def get_num_lands_from_dataset(h5_file_path):\n",
    "    f = h5.File(h5_file_path, 'r')\n",
    "    \n",
    "    num_lands = int(f['land-names/num-lands'][()])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return num_lands\n",
    "\n",
    "def get_land_names_from_dataset(h5_file_path):\n",
    "    f = h5.File(h5_file_path, 'r')\n",
    "    \n",
    "    num_lands = int(f['land-names/num-lands'][()])\n",
    "\n",
    "    land_names = []\n",
    "\n",
    "    for l in range(num_lands):\n",
    "        s = f['land-names/land-{:02d}'.format(l)][()]\n",
    "        if (type(s) is bytes) or (type(s) is np.bytes_):\n",
    "            s = s.decode()\n",
    "        assert(type(s) is str)\n",
    "\n",
    "        land_names.append(s)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    return land_names\n",
    "\n",
    "\n",
    "def get_dataset(h5_file_path, pat_inds, num_classes,\n",
    "                pad_img_dim=0, no_seg=False,\n",
    "                minmax=None,\n",
    "                data_aug=False,\n",
    "                train_valid_split=None,\n",
    "                train_valid_idx=None,\n",
    "                dup_data_w_left_right_flip=False):\n",
    "    # classes:\n",
    "    # 0 --> BG\n",
    "    # 1 --> Pelvis\n",
    "    # 2 --> Left Femur\n",
    "    # 3 --> Right Femur\n",
    "        \n",
    "    need_to_scale_data   = False\n",
    "    need_to_find_min_max = False\n",
    "\n",
    "    if minmax is not None:\n",
    "        if (type(minmax) is bool) and minmax:\n",
    "            need_to_scale_data = True\n",
    "            print('need to find min/max for preprocessing...')\n",
    "            need_to_find_min_max = True\n",
    "            minmax_min =  math.inf\n",
    "            minmax_max = -math.inf\n",
    "        elif type(minmax) is tuple:\n",
    "            minmax_min = minmax[0]\n",
    "            minmax_max = minmax[1]\n",
    "            need_to_scale_data = True\n",
    "            print('using provided min/max for preprocessing: ({}, {})'.format(minmax_min, minmax_max))\n",
    "\n",
    "    f = h5.File(h5_file_path, 'r')\n",
    "\n",
    "    all_projs = None\n",
    "    all_segs  = None\n",
    "    all_lands = None\n",
    "\n",
    "    orig_img_shape = None\n",
    "\n",
    "    for pat_idx in pat_inds:\n",
    "        pat_g = f['{:02d}'.format(pat_idx)]\n",
    "\n",
    "        cur_projs_np = pat_g['projs'][:]\n",
    "        assert(len(cur_projs_np.shape) == 3)\n",
    "        print(f\"cur_projs_np shape={cur_projs_np.shape}\")\n",
    "\n",
    "        if orig_img_shape is None:\n",
    "            orig_img_shape = (cur_projs_np.shape[1], cur_projs_np.shape[2])#180*180\n",
    "        else:\n",
    "            assert(orig_img_shape[0] == cur_projs_np.shape[1])\n",
    "            assert(orig_img_shape[1] == cur_projs_np.shape[2])\n",
    "        \n",
    "        cur_lands = torch.from_numpy(pat_g['lands'][:])\n",
    "        assert(cur_lands.shape[0] == cur_projs_np.shape[0])\n",
    "        assert(torch.all(torch.isfinite(cur_lands)))  # all inputs should be finite\n",
    "\n",
    "        print(f\"cur_lands shape={cur_lands.shape}\")#Size([111, 2, 14])\n",
    "        print(f\"orig_img_shape shape=({orig_img_shape[0]},{orig_img_shape[1]})\")\n",
    "\n",
    "        # mark out of bounds landmarks with inf's\n",
    "        #不合法的数据直接设为math.inf\n",
    "        for img_idx in range(cur_lands.shape[0]):\n",
    "            for l_idx in range(cur_lands.shape[-1]):\n",
    "                cur_l = cur_lands[img_idx,:,l_idx]\n",
    "\n",
    "                if (cur_l[0] < 0) or (cur_l[0] > (orig_img_shape[1]-1)) or \\\n",
    "                   (cur_l[1] < 0) or (cur_l[1] > (orig_img_shape[0]-1)):\n",
    "                       cur_l[0] = math.inf\n",
    "                       cur_l[1] = math.inf\n",
    "\n",
    "        if need_to_find_min_max:\n",
    "            minmax_min = min(minmax_min, cur_projs_np.min())\n",
    "            minmax_max = max(minmax_max, cur_projs_np.max())\n",
    "\n",
    "        cur_projs = torch.from_numpy(cur_projs_np)\n",
    "        print(f\"cur_projs shape={cur_projs.shape}\")#[111, 180, 180]\n",
    "\n",
    "        # Need a singleton dimension to represent grayscale data\n",
    "        cur_projs = cur_projs.view(cur_projs.shape[0], 1, cur_projs.shape[1], cur_projs.shape[2])\n",
    "        print(f\"cur_projs shape={cur_projs.shape}\")#([111, 1, 180, 180])\n",
    "\n",
    "        if all_projs is None:\n",
    "            all_projs = cur_projs\n",
    "        else:\n",
    "            all_projs = torch.cat((all_projs, cur_projs))\n",
    "\n",
    "        cur_segs = torch.from_numpy(pat_g['segs'][:])\n",
    "        assert(len(cur_segs.shape) == 3)\n",
    "        print(f\"cur_segs shape={cur_segs.shape}\")#111, 180, 180\n",
    "        print(f\"all_projs shape={all_projs.shape}\")#111, 1, 180, 180\n",
    "\n",
    "        cur_segs_dice = torch.zeros(cur_segs.shape[0], num_classes, cur_segs.shape[1], cur_segs.shape[2])\n",
    "        print(f\"cur_segs_dice shape={cur_segs_dice.shape}\")#[111, 7, 180, 180]\n",
    "        \n",
    "        #设置dice [True,False]\n",
    "        for i in range(cur_segs.shape[0]):\n",
    "            for c in range(num_classes):\n",
    "                cur_segs_dice[i,c,:,:] = cur_segs[i,:,:] == c\n",
    "\n",
    "        if all_segs is None:\n",
    "            all_segs = cur_segs_dice.clone().detach()\n",
    "        else:\n",
    "            all_segs = torch.cat((all_segs, cur_segs_dice))\n",
    "\n",
    "        print(f\"all_segs shape={all_segs.shape}\")\n",
    "\n",
    "        if all_lands is None:\n",
    "            all_lands = cur_lands.clone().detach()\n",
    "        else:\n",
    "            all_lands = torch.cat((all_lands, cur_lands))\n",
    "\n",
    "        if dup_data_w_left_right_flip:#默认是Fasle\n",
    "            all_projs = torch.cat((all_projs, torch.flip(cur_projs, [3])))\n",
    "\n",
    "            # left/right flip the segmentations\n",
    "            cur_segs_dice = torch.flip(cur_segs_dice, [3])\n",
    "\n",
    "            assert(cur_segs_dice.shape[1] == 7)  # TODO: allow for a mapping to be passed\n",
    "            # update l/r labels\n",
    "            # 0 BG stays the same\n",
    "            # 1 left hemipelvis <--> 2 right hemipelvis\n",
    "            # 3 vertebrae stays the same\n",
    "            # 4 upper sacrum stays the smae\n",
    "            # 5 left femur <--> 6 left femur\n",
    "\n",
    "            def swap_classes(c1, c2):\n",
    "                tmp_copy  = cur_segs_dice[:,c1,:,:].clone().detach()\n",
    "                cur_segs_dice[:,c1,:,:] = cur_segs_dice[:,c2,:,:]\n",
    "                cur_segs_dice[:,c2,:,:] = tmp_copy\n",
    "\n",
    "            swap_classes(1,2)\n",
    "            swap_classes(5,6)\n",
    "\n",
    "            # flip lands and update, etc\n",
    "            for img_idx in range(cur_lands.shape[0]):\n",
    "                # do the l/r flip for each landmark\n",
    "                for l_idx in range(cur_lands.shape[-1]):\n",
    "                    cur_l = cur_lands[img_idx,:,l_idx]\n",
    "                    if math.isfinite(cur_l[0]) and math.isfinite(cur_l[1]):\n",
    "                        cur_l[0] = (orig_img_shape[-1] - 1) - cur_l[0]\n",
    "                \n",
    "                # now swap the l/r landmarks\n",
    "                assert((cur_lands.shape[-1] % 2) == 0)\n",
    "                for l_idx in range(cur_lands.shape[-1] // 2):\n",
    "                    tmp_land = cur_lands[img_idx,:,l_idx].clone().detach()\n",
    "                    cur_lands[img_idx,:,l_idx] = cur_lands[img_idx,:,l_idx+1]\n",
    "                    cur_lands[img_idx,:,l_idx] = tmp_land\n",
    "            \n",
    "            all_segs = torch.cat((all_segs, cur_segs_dice))\n",
    "            all_lands = torch.cat((all_lands, cur_lands))\n",
    "    \n",
    "    # end loop over patients\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    # scale to [0,1] if needed\n",
    "    if need_to_scale_data:\n",
    "        assert((minmax_max - minmax_min) > 1.0e-6)\n",
    "        print('scaling data using min/max: {} , {}'.format(minmax_min, minmax_max))\n",
    "        all_projs = (all_projs - minmax_min) / (minmax_max - minmax_min)\n",
    "    \n",
    "    def set_helper_vars(ds, do_data_aug):\n",
    "        ds.prob_of_aug = 0.5 if do_data_aug else 0.0\n",
    "        \n",
    "        # stuff in some custom vars\n",
    "        ds.rob_orig_img_shape = orig_img_shape\n",
    "\n",
    "        ds.rob_data_is_scaled = need_to_scale_data\n",
    "        if need_to_scale_data:\n",
    "            ds.rob_minmax = (minmax_min, minmax_max)\n",
    "\n",
    "    if (train_valid_split is not None) and (train_valid_split > 0):\n",
    "        print('split dataset into train/validation')\n",
    "        assert((0.0 < train_valid_split) and (train_valid_split < 1.0))\n",
    "        num_train = int(math.ceil(train_valid_split * all_projs.shape[0]))\n",
    "        num_valid = all_projs.shape[0] - num_train\n",
    "\n",
    "        all_inds = list(range(all_projs.shape[0]))\n",
    "        \n",
    "        if (train_valid_idx is None) or (train_valid_idx[0] is None) or (train_valid_idx[1] is None):\n",
    "            print('  randomly splitting all complete tensors into training/validation...')\n",
    "            random.shuffle(all_inds)\n",
    "\n",
    "            train_inds = all_inds[:num_train]\n",
    "            valid_inds = all_inds[num_train:]\n",
    "        else:\n",
    "            print('  use previously specified split')\n",
    "            train_inds = train_valid_idx[0]\n",
    "            valid_inds = train_valid_idx[1]\n",
    "            assert(len(train_inds) == num_train)\n",
    "            assert(len(valid_inds) == num_valid)\n",
    "        \n",
    "        train_ds = RandomDataAugDataSet(all_projs[train_inds,:,:,:], all_segs[train_inds,:,:,:], all_lands[train_inds,:,:], proj_pad_dim=pad_img_dim)\n",
    "        set_helper_vars(train_ds, data_aug)\n",
    "        \n",
    "        valid_ds = RandomDataAugDataSet(all_projs[valid_inds,:,:,:], all_segs[valid_inds,:,:,:], all_lands[valid_inds,:,:], proj_pad_dim=pad_img_dim)\n",
    "        set_helper_vars(valid_ds, False)\n",
    "\n",
    "        return (train_ds, valid_ds, train_inds, valid_inds)\n",
    "    else:\n",
    "        ds = RandomDataAugDataSet(all_projs, all_segs, all_lands, proj_pad_dim=pad_img_dim)\n",
    "        set_helper_vars(ds, data_aug)\n",
    "\n",
    "        return ds\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#Log transform\n",
    "def ogarithmic_transformation(input_image:np.ndarray, c=1, inplace = True):\n",
    "    '''\n",
    "    对数变换\n",
    "    :param input_image: 原图像\n",
    "    :param c: 对数变换超参数\n",
    "    :return: 对数变换后的图像\n",
    "    '''\n",
    "    # input_image_np = np.copy(input_image)\n",
    "    input_image_cp = input_image\n",
    "    if not inplace:\n",
    "        input_image_cp = input_image.copy()\n",
    "        \n",
    "    output_imgae = c * np.log(1 + input_image_cp) # 输出图像\n",
    "\n",
    "    return output_imgae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def prepare_data(cur_proj_np:np.ndarray):\n",
    "    \n",
    "    if USE_PAD_DATA:\n",
    "        pad_x_size = 1536-cur_proj_np.shape[0]\n",
    "        pad_y_size = 1536-cur_proj_np.shape[1]\n",
    "        cur_proj_np = np.pad(cur_proj_np,((0,pad_x_size),(0,pad_y_size)),'constant', constant_values=0)\n",
    "    \n",
    "    #1.剪裁\n",
    "    cur_proj_np = cur_proj_np[50:-50,50:-50]\n",
    "\n",
    "    # 2.下采样\n",
    "    # Since the projections are 1536x1536, a tiled image of ~100 projections\n",
    "    # may be excessively large, downsample in this case\n",
    "    #overlay_ds_factor = 1.0   # no downsampling\n",
    "    overlay_ds_factor = 0.125 # downsample 8x in each 2D dim\n",
    "\n",
    "    need_to_ds_overlay = abs(overlay_ds_factor - 1.0) > 0.001\n",
    "\n",
    "    (proj_num_cols,proj_num_rows) = cur_proj_np.shape\n",
    "\n",
    "    # downsampled overlay dimensions\n",
    "    ds_proj_num_cols = int(round(proj_num_cols * overlay_ds_factor)) if need_to_ds_overlay else proj_num_cols\n",
    "    ds_proj_num_rows = int(round(proj_num_rows * overlay_ds_factor)) if need_to_ds_overlay else proj_num_rows\n",
    "\n",
    "    pil = TF.to_pil_image(cur_proj_np)\n",
    "    if need_to_ds_overlay:\n",
    "        pil = pil.resize((ds_proj_num_cols, ds_proj_num_rows), Image.BILINEAR)\n",
    "        # cur_proj = TF.to_tensor(pil)\n",
    "        # print(f\"cur_proj shape:{cur_proj.shape}\")\n",
    "    # else:\n",
    "    cur_proj_np = np.array(pil)\n",
    "    print(f\"cur dtype:{cur_proj_np.dtype}\")\n",
    "    # #show\n",
    "    # plt.imshow(pil,cmap='gray')\n",
    "    # plt.show()\n",
    "    \n",
    "    #3.log transform\n",
    "    cur_proj_np = ogarithmic_transformation(cur_proj_np,1)\n",
    "    #cur_proj = torch.log(1+cur_proj)\n",
    "  \n",
    "\n",
    "    #数据需要归一化\n",
    "    #找出最小值\n",
    "    # minmax_min =  math.inf\n",
    "    # minmax_max = -math.inf\n",
    "    # minmax_min = min(minmax_min, cur_proj_np.min())\n",
    "    # minmax_max = max(minmax_max, cur_proj_np.max())\n",
    "    # print(f\"max:{minmax_max},min:{minmax_min}\")\n",
    "    \n",
    "    \n",
    "    # #归一化操作\n",
    "    # assert((minmax_max - minmax_min) > 1.0e-6)\n",
    "    # print('scaling data using min/max: {} , {}'.format(minmax_min, minmax_max))\n",
    "    # cur_proj_np = (cur_proj_np - minmax_min) / (minmax_max - minmax_min)\n",
    "\n",
    "\n",
    "    #4.pading 到192x192\n",
    "    cur_proj = torch.from_numpy(np.pad(cur_proj_np,\n",
    "                                 ( (6,6), (6,6)),\n",
    "                                 'reflect'))\n",
    "    # cur_proj = TF.pad(cur_proj,(0,6,6),0,'reflect')\n",
    "    cur_proj = (cur_proj - cur_proj.mean()) / cur_proj.std()\n",
    "\n",
    "    x = torch.unsqueeze(cur_proj,0)\n",
    "    x = torch.unsqueeze(x,0).type(torch.FloatTensor)  # 转Float\n",
    "    print(f\"cur_proj shape:{x.shape}\\n cur_proj dtype:{x.dtype}\")\n",
    "\n",
    "    do_invert = True\n",
    "    do_gamma  = True\n",
    "    do_noise  = True\n",
    "    do_affine = True\n",
    "    do_erase  = True\n",
    "\n",
    "\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def prepare_data_301(cur_proj_np:np.ndarray):\n",
    "    \n",
    "    #1.剪裁\n",
    "    cur_proj_np = cur_proj_np[50:-50,50:-50]\n",
    "\n",
    "    # 2.下采样\n",
    "    # Since the projections are 1536x1536, a tiled image of ~100 projections\n",
    "    # may be excessively large, downsample in this case\n",
    "    #overlay_ds_factor = 1.0   # no downsampling\n",
    "    overlay_ds_factor = 0.125 # downsample 8x in each 2D dim\n",
    "\n",
    "    need_to_ds_overlay = abs(overlay_ds_factor - 1.0) > 0.001\n",
    "\n",
    "    (proj_num_cols,proj_num_rows) = cur_proj_np.shape\n",
    "\n",
    "    # downsampled overlay dimensions\n",
    "    ds_proj_num_cols = int(round(proj_num_cols * overlay_ds_factor)) if need_to_ds_overlay else proj_num_cols\n",
    "    ds_proj_num_rows = int(round(proj_num_rows * overlay_ds_factor)) if need_to_ds_overlay else proj_num_rows\n",
    "\n",
    "    ds_proj_num_cols = ds_proj_num_cols if ds_proj_num_cols % 2 == 0 else ds_proj_num_cols - 1\n",
    "    ds_proj_num_rows = ds_proj_num_rows if ds_proj_num_rows % 2 == 0 else ds_proj_num_rows - 1\n",
    "\n",
    "\n",
    "    pil = TF.to_pil_image(cur_proj_np)\n",
    "    if need_to_ds_overlay:\n",
    "        pil = pil.resize((ds_proj_num_cols, ds_proj_num_rows), Image.BILINEAR)\n",
    "        # cur_proj = TF.to_tensor(pil)\n",
    "        # print(f\"cur_proj shape:{cur_proj.shape}\")\n",
    "    # else:\n",
    "    cur_proj_np = np.array(pil)\n",
    "    print(f\"cur dtype:{cur_proj_np.dtype}\")\n",
    "    # #show\n",
    "    # plt.imshow(pil,cmap='gray')\n",
    "    # plt.show()\n",
    "    \n",
    "    #3.log transform\n",
    "    cur_proj_np = ogarithmic_transformation(cur_proj_np,1)\n",
    "    #cur_proj = torch.log(1+cur_proj)\n",
    "  \n",
    "\n",
    "    #数据需要归一化\n",
    "    #找出最小值\n",
    "    # minmax_min =  math.inf\n",
    "    # minmax_max = -math.inf\n",
    "    # minmax_min = min(minmax_min, cur_proj_np.min())\n",
    "    # minmax_max = max(minmax_max, cur_proj_np.max())\n",
    "    # print(f\"max:{minmax_max},min:{minmax_min}\")\n",
    "    \n",
    "    \n",
    "    # #归一化操作\n",
    "    # assert((minmax_max - minmax_min) > 1.0e-6)\n",
    "    # print('scaling data using min/max: {} , {}'.format(minmax_min, minmax_max))\n",
    "    # cur_proj_np = (cur_proj_np - minmax_min) / (minmax_max - minmax_min)\n",
    "     \n",
    "    extra_pad_x = calc_pad_amount(192, cur_proj_np.shape[-2])\n",
    "    extra_pad_y = calc_pad_amount(192, cur_proj_np.shape[-1])\n",
    "\n",
    "    print(f\"({extra_pad_x},{extra_pad_y})\")\n",
    "\n",
    "    # return\n",
    "\n",
    "    #4.pading 到192x192\n",
    "    cur_proj = torch.from_numpy(np.pad(cur_proj_np,\n",
    "                                 ( (extra_pad_x,extra_pad_x), (extra_pad_y,extra_pad_y)),\n",
    "                                 'reflect'))\n",
    "    # cur_proj = TF.pad(cur_proj,(0,6,6),0,'reflect')\n",
    "    cur_proj = (cur_proj - cur_proj.mean()) / cur_proj.std()\n",
    "\n",
    "    x = torch.unsqueeze(cur_proj,0)\n",
    "    x = torch.unsqueeze(x,0).type(torch.FloatTensor)  # 转Float\n",
    "    print(f\"cur_proj shape:{x.shape}\\n cur_proj dtype:{x.dtype}\")\n",
    "    \n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def prepare_data_301_2(cur_proj_np:np.ndarray):\n",
    "    \n",
    "    #1.剪裁\n",
    "    crop_size_h = (int)(abs(cur_proj_np.shape[-2] - 960)/2)\n",
    "    crop_size_w = (int)(abs(cur_proj_np.shape[-1] - 960)/2)\n",
    "    print(f\"crop_size({crop_size_h,crop_size_w})\")\n",
    "    cur_proj_np = cur_proj_np[crop_size_h:-crop_size_h-1,crop_size_w:-crop_size_w]\n",
    "    print(f\"cur_proj_np({cur_proj_np.shape})\")\n",
    "\n",
    "    # 2.下采样\n",
    "    # Since the projections are 1536x1536, a tiled image of ~100 projections\n",
    "    # may be excessively large, downsample in this case\n",
    "    #overlay_ds_factor = 1.0   # no downsampling\n",
    "    overlay_ds_factor = 0.2 # downsample 5x in each 2D dim\n",
    "\n",
    "    need_to_ds_overlay = abs(overlay_ds_factor - 1.0) > 0.001\n",
    "\n",
    "    (proj_num_cols,proj_num_rows) = cur_proj_np.shape\n",
    "\n",
    "    # downsampled overlay dimensions\n",
    "    ds_proj_num_cols = int(round(proj_num_cols * overlay_ds_factor)) if need_to_ds_overlay else proj_num_cols\n",
    "    ds_proj_num_rows = int(round(proj_num_rows * overlay_ds_factor)) if need_to_ds_overlay else proj_num_rows\n",
    "\n",
    "    ds_proj_num_cols = ds_proj_num_cols if ds_proj_num_cols % 2 == 0 else ds_proj_num_cols - 1\n",
    "    ds_proj_num_rows = ds_proj_num_rows if ds_proj_num_rows % 2 == 0 else ds_proj_num_rows - 1\n",
    "    print(cur_proj_np.shape,ds_proj_num_cols,ds_proj_num_rows)\n",
    "\n",
    "\n",
    "    pil = TF.to_pil_image(cur_proj_np)\n",
    "    if need_to_ds_overlay:\n",
    "        pil = pil.resize((ds_proj_num_cols, ds_proj_num_rows), Image.BILINEAR)\n",
    "        # cur_proj = TF.to_tensor(pil)\n",
    "        # print(f\"cur_proj shape:{cur_proj.shape}\")\n",
    "    # else:\n",
    "    cur_proj_np = np.array(pil)\n",
    "    print(f\"cur dtype:{cur_proj_np.dtype}\")\n",
    "    # #show\n",
    "    # plt.imshow(pil,cmap='gray')\n",
    "    # plt.show()\n",
    "    \n",
    "    #3.log transform\n",
    "    cur_proj_np = ogarithmic_transformation(cur_proj_np,1)\n",
    "    #cur_proj = torch.log(1+cur_proj)\n",
    "  \n",
    "\n",
    "    #数据需要归一化\n",
    "    #找出最小值\n",
    "    # minmax_min =  math.inf\n",
    "    # minmax_max = -math.inf\n",
    "    # minmax_min = min(minmax_min, cur_proj_np.min())\n",
    "    # minmax_max = max(minmax_max, cur_proj_np.max())\n",
    "    # print(f\"max:{minmax_max},min:{minmax_min}\")\n",
    "    \n",
    "    \n",
    "    # #归一化操作\n",
    "    # assert((minmax_max - minmax_min) > 1.0e-6)\n",
    "    # print('scaling data using min/max: {} , {}'.format(minmax_min, minmax_max))\n",
    "    # cur_proj_np = (cur_proj_np - minmax_min) / (minmax_max - minmax_min)\n",
    "    if cur_proj_np.shape[-2] < 192: \n",
    "        extra_pad_x = calc_pad_amount(192, cur_proj_np.shape[-2])\n",
    "        extra_pad_y = calc_pad_amount(192, cur_proj_np.shape[-1])\n",
    "\n",
    "        print(f\"({extra_pad_x},{extra_pad_y})\")\n",
    "\n",
    "        # return\n",
    "\n",
    "        #4.pading 到192x192\n",
    "        cur_proj = torch.from_numpy(np.pad(cur_proj_np,\n",
    "                                    ( (extra_pad_x,extra_pad_x), (extra_pad_y,extra_pad_y)),\n",
    "                                    'reflect'))\n",
    "        # cur_proj = TF.pad(cur_proj,(0,6,6),0,'reflect')\n",
    "    else:\n",
    "        cur_proj = torch.from_numpy(cur_proj_np)\n",
    "        \n",
    "    cur_proj = (cur_proj - cur_proj.mean()) / cur_proj.std()\n",
    "\n",
    "    x = torch.unsqueeze(cur_proj,0)\n",
    "    x = torch.unsqueeze(x,0).type(torch.FloatTensor)  # 转Float\n",
    "    print(f\"cur_proj shape:{x.shape}\\n cur_proj dtype:{x.dtype}\")\n",
    "    \n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 单个数据的推断"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def seg_single_data_ensemble(data,net,h5_f,dev=None,num_lands=0):\n",
    "    \n",
    "    if USE_MY_DETE:\n",
    "        orig_img_shape = (192,192)\n",
    "    else:\n",
    "        orig_img_shape = (180,180)\n",
    "\n",
    "    dst_ds = h5_f.create_dataset('nn-segs', (1, *orig_img_shape),\n",
    "                                 dtype='u1',\n",
    "                                 chunks=(1, *orig_img_shape),\n",
    "                                 compression=\"gzip\", compression_opts=9)\n",
    "    if num_lands > 0:\n",
    "        dst_heats_ds = h5_f.create_dataset('nn-heats', (1, num_lands, *orig_img_shape),\n",
    "                                        chunks=(1,1,*orig_img_shape),\n",
    "                                        compression=\"gzip\", compression_opts=9)  \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        input = data.to(dev)\n",
    "        net_out = net(input)\n",
    "        if (num_lands > 0) or (type(net_out) is tuple):\n",
    "            pred_masks = net_out[0]\n",
    "            pred_heats = net_out[1]  \n",
    "        else:\n",
    "            pred_masks = net_out\n",
    "        \n",
    "        print(f\"pred_mask shape:{pred_masks.shape}\")\n",
    "        pred_masks = center_crop(pred_masks, orig_img_shape)\n",
    "\n",
    "        if dst_heats_ds is not None:\n",
    "            pred_heats = center_crop(pred_heats, orig_img_shape)\n",
    "            \n",
    "            pred_heats_min = pred_heats.min().item()\n",
    "            pred_heats_max = pred_heats.max().item()\n",
    "            \n",
    "            pred_heats = (pred_heats - pred_heats_min) / (pred_heats_max - pred_heats_min)\n",
    "\n",
    "\n",
    "        (_, pred_masks) = torch.max(pred_masks, dim=1) \n",
    "\n",
    "        # write to file\n",
    "        dst_ds[0,:,:] = pred_masks.view(orig_img_shape).cpu().numpy()\n",
    "        dst_heats_ds[0,:,:,:] = pred_heats.cpu().numpy()                   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from unet    import *\n",
    "#python -u test_ensemble.py ipcai_2020_ds_8x.h5 spec_1_test.h5 --pats 1 --nets yy_best_net.pt\n",
    "# os.path.pardir('')\n",
    "torch_map_loc = None\n",
    "network_paths=list()\n",
    "\n",
    "network_paths.append(os.path.join(cur_path,'yy_best_net.pt'))  \n",
    "print(network_paths)\n",
    "\n",
    "cpu_dev = torch.device('cpu')\n",
    "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch_map_loc = dev\n",
    "print(f\"dev type:{dev}\")\n",
    "\n",
    "nets=[]\n",
    "for net_path in network_paths:\n",
    "        print('  loading state from disk for: {}'.format(net_path))\n",
    "        \n",
    "        state = torch.load(net_path, map_location=torch_map_loc)\n",
    "        \n",
    "        print('  loading unet params from checkpoint state dict...')\n",
    "        num_classes         = state['num-classes']\n",
    "        unet_num_lvls       = state['depth']\n",
    "        unet_init_feats_exp = state['init-feats-exp']\n",
    "        unet_batch_norm     = state['batch-norm']\n",
    "        unet_padding        = state['padding']\n",
    "        unet_no_max_pool    = state['no-max-pool']\n",
    "        unet_use_res        = state['unet-use-res']\n",
    "        unet_block_depth    = state['unet-block-depth']\n",
    "        proj_unet_dim       = state['pad-img-size']\n",
    "        batch_size          = state['batch-size']\n",
    "        num_lands           = state['num-lands']\n",
    "        epoch               = state['epoch']\n",
    "        loss                = state['loss']\n",
    "        best_valid_loss     = state['best-valid-loss']\n",
    "\n",
    "        print('             num. classes: {}'.format(num_classes))\n",
    "        print('                    depth: {}'.format(unet_num_lvls))\n",
    "        print('        init. feats. exp.: {}'.format(unet_init_feats_exp))\n",
    "        print('              batch norm.: {}'.format(unet_batch_norm))\n",
    "        print('         unet do pad img.: {}'.format(unet_padding))\n",
    "        print('              no max pool: {}'.format(unet_no_max_pool))\n",
    "        print('    reflect pad img. dim.: {}'.format(proj_unet_dim))\n",
    "        print('            unet use res.: {}'.format(unet_use_res))\n",
    "        print('         unet block depth: {}'.format(unet_block_depth))\n",
    "        print('               batch size: {}'.format(batch_size))\n",
    "        print('              num. lands.: {}'.format(num_lands))\n",
    "        \n",
    "        print('          Last Epoch: {}'.format(epoch))\n",
    "        print('           Last Loss: {}'.format(loss.item()))\n",
    "        print('    Best Valid. Loss: {}'.format(best_valid_loss))\n",
    "\n",
    "        print('    creating network')\n",
    "        net = UNet(n_classes=num_classes, depth=unet_num_lvls, wf=unet_init_feats_exp, batch_norm=unet_batch_norm, padding=unet_padding, max_pool=not unet_no_max_pool,\n",
    "                   num_lands=num_lands, do_res=unet_use_res, block_depth=unet_block_depth)\n",
    "    \n",
    "        net.load_state_dict(state['model-state-dict'])\n",
    "\n",
    "        del state\n",
    "\n",
    "        print('  moving network to device...')\n",
    "        net.to(dev)\n",
    "        \n",
    "        nets.append(net)\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['/home/xindong/project/rg2-work/yy_best_net.pt']\n",
      "dev type:cuda:0\n",
      "  loading state from disk for: /home/xindong/project/rg2-work/yy_best_net.pt\n",
      "  loading unet params from checkpoint state dict...\n",
      "             num. classes: 7\n",
      "                    depth: 6\n",
      "        init. feats. exp.: 5\n",
      "              batch norm.: True\n",
      "         unet do pad img.: True\n",
      "              no max pool: True\n",
      "    reflect pad img. dim.: 192\n",
      "            unet use res.: True\n",
      "         unet block depth: 2\n",
      "               batch size: 5\n",
      "              num. lands.: 14\n",
      "          Last Epoch: 484\n",
      "           Last Loss: -0.7985647916793823\n",
      "    Best Valid. Loss: -0.8046208620071411\n",
      "    creating network\n",
      "  moving network to device...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "if USE_RESAMPLE_DATA:\n",
    "        src_data_file_path = os.path.join(data_path,'ipcai_2020_ds_8x.h5')\n",
    "        dst_data_file_path = os.path.join(data_path,'spec_301_resample.h5')\n",
    "elif USE_PAD_DATA:\n",
    "        src_data_file_path = os.path.join(data_path,'ipcai_2020_ds_8x.h5')\n",
    "        dst_data_file_path = os.path.join(data_path,'spec_301_pad.h5')\n",
    "\n",
    "elif USE_MY_DETE:\n",
    "        src_data_file_path = os.path.join(data_path,'my_source_ds.h5')\n",
    "        dst_data_file_path = os.path.join(data_path,'spec_301_test_4.h5')\n",
    "else:\n",
    "        src_data_file_path = os.path.join(data_path,'ipcai_2020_ds_8x.h5')\n",
    "        dst_data_file_path = os.path.join(data_path,'spec_1_test_bxd.h5')\n",
    "        \n",
    "test_pats =[]\n",
    "test_pats.append(1)\n",
    "\n",
    "land_names = None\n",
    "if num_lands > 0:\n",
    "        land_names = get_land_names_from_dataset(src_data_file_path)\n",
    "        assert(len(land_names) == num_lands)\n",
    "\n",
    "# print('initializing testing dataset')\n",
    "# test_ds = get_dataset(src_data_file_path, test_pats, num_classes=num_classes,\n",
    "#                           pad_img_dim=proj_unet_dim, no_seg=True)\n",
    "\n",
    "print('initializing testing data')\n",
    "if not USE_MY_DETE:\n",
    "        xray_data = prepare_data(xray_data) \n",
    "else:\n",
    "        xray_data = prepare_data_301_2(xray_data) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initializing testing data\n",
      "cur dtype:float32\n",
      "cur_proj shape:torch.Size([1, 1, 192, 192])\n",
      " cur_proj dtype:torch.float32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# print('Length of testing dataset: {}'.format(len(test_ds)))\n",
    "\n",
    "print('opening destination file for writing')\n",
    "f = h5.File(dst_data_file_path, 'w')\n",
    "\n",
    "# save off the landmark names\n",
    "if land_names:\n",
    "        land_names_g = f.create_group('land-names')\n",
    "        land_names_g['num-lands'] = num_lands\n",
    "\n",
    "for l in range(num_lands):\n",
    "        land_names_g['land-{:02d}'.format(l)] = land_names[l]\n",
    "\n",
    "times = []\n",
    "\n",
    "print('running network on projections')\n",
    "# seg_dataset_ensemble(test_ds, nets, f, dev=dev, num_lands=num_lands, times=times)\n",
    "\n",
    "seg_single_data_ensemble(xray_data,nets[0],f,dev=dev,num_lands=num_lands)\n",
    "\n",
    "print('closing file...')\n",
    "f.flush()\n",
    "f.close()\n",
    "\n",
    "test_time=''\n",
    "if test_time:\n",
    "        times_out = open(test_time, 'w')\n",
    "        for t in times:\n",
    "                times_out.write('{:.6f}\\n'.format(t))\n",
    "        times_out.flush()\n",
    "        times_out.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "opening destination file for writing\n",
      "running network on projections\n",
      "pred_mask shape:torch.Size([1, 7, 192, 192])\n",
      "closing file...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2020 Robert Grupp\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# This program extracts anatomical landmark locations, for a single projection,\n",
    "# that were inferred by a neural network and saved in CSV format.\n",
    "# See https://github.com/rg2/DeepFluoroLabeling-IPCAI2020/tree/master/train_test_code\n",
    "# The output file format is FCSV, which is a 3D Slicer and xReg compatible file format.\n",
    "# This allows the estimated landmarks to be used for the intraoperative registration strategy.\n",
    "# A downsampling factor needs to be passed which matches the amount of downsampling applied \n",
    "# to the full-resolution projections to create training data. (e.g. 0.125 for 8x downsampling).\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "def write_lands_map_to_fcsv(lands, dst_fcsv_path, lps_to_ras=True):\n",
    "    with open(dst_fcsv_path, 'w') as f:\n",
    "        f.write('# Markups fiducial file version = 4.6\\n')\n",
    "        f.write('# CoordinateSystem = 0\\n')\n",
    "        f.write('# columns = id,x,y,z,ow,ox,oy,oz,vis,sel,lock,label,desc,associatedNodeID\\n')\n",
    "\n",
    "        for (land_name, land_pt) in lands.items():\n",
    "            x = land_pt[0]\n",
    "            y = land_pt[1]\n",
    "            z = land_pt[2]\n",
    "\n",
    "            if lps_to_ras:\n",
    "                x *= -1\n",
    "                y *= -1\n",
    "\n",
    "            f.write(',{:.8f},{:.8f},{:.8f},0,0,0,1,1,1,0,{},,\\n'.format(x,y,z,land_name))\n",
    "\n",
    "        f.flush()\n",
    "\n",
    "def extract_lands_map_for_proj_from_nn_csv(src_nn_csv_path, pat_idx, proj_idx, ds_factor):\n",
    "    kLAND_NAMES = [ 'FH-l',   'FH-r',\n",
    "                    'GSN-l',  'GSN-r',\n",
    "                    'IOF-l',  'IOF-r',\n",
    "                    'MOF-l',  'MOF-r',\n",
    "                    'SPS-l',  'SPS-r',\n",
    "                    'IPS-l',  'IPS-r',\n",
    "                    'ASIS-l', 'ASIS-r']\n",
    "    if USE_MY_DETE:\n",
    "        kCROP_WIDTH_FULL_RES_PIX = 9\n",
    "        kCROP_HIGH_FULL_RES_PIX = 53\n",
    "        kORIG_PIX_SPACING = 0.154\n",
    "\n",
    "    elif USE_PAD_DATA:\n",
    "        kCROP_WIDTH_FULL_RES_PIX = 50\n",
    "        kCROP_HIGH_FULL_RES_PIX = 50\n",
    "        kORIG_PIX_SPACING = 0.154\n",
    "\n",
    "    elif USE_RESAMPLE_DATA:\n",
    "        kCROP_WIDTH_FULL_RES_PIX = 50\n",
    "        kCROP_HIGH_FULL_RES_PIX = 50\n",
    "        kORIG_PIX_SPACING = 0.098\n",
    "        \n",
    "    else:\n",
    "        kCROP_WIDTH_FULL_RES_PIX = 50\n",
    "        kCROP_HIGH_FULL_RES_PIX = 50\n",
    "        kORIG_PIX_SPACING = 0.194\n",
    "\n",
    "    lands_map = { }\n",
    "\n",
    "    with open(src_nn_csv_path) as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        \n",
    "        for csv_row in csv_reader:\n",
    "            if (int(csv_row['pat']) == pat_idx) and (int(csv_row['proj']) == proj_idx):\n",
    "                r = int(csv_row['row'])\n",
    "                c = int(csv_row['col'])\n",
    "\n",
    "                if (r >= 0) and (c >= 0):\n",
    "                    # y = ((r / ds_factor) + kCROP_WIDTH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "                    # x = ((c / ds_factor) + kCROP_WIDTH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "\n",
    "                    # 看着不太对\n",
    "                    # y = ((r / ds_factor) + kCROP_WIDTH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "                    # x = ((c / ds_factor) + kCROP_HIGH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "\n",
    "                    y = ((r / ds_factor) + kCROP_HIGH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "                    x = ((c / ds_factor) + kCROP_WIDTH_FULL_RES_PIX) * kORIG_PIX_SPACING\n",
    "\n",
    "\n",
    "                    lands_map[kLAND_NAMES[int(csv_row['land'])]] = (x,y,0.0)\n",
    "\n",
    "    return lands_map\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "python est_lands_csv.py data/spec_1_test_bxd.h5 nn-heats --use-seg nn-segs --pat 1 --out spec_1-6_lands.csv   \n",
    "python est_lands_csv.py data/spec_301_test_4.h5  nn-heats --use-seg nn-segs --pat 1 --out spec_301_lands_930.csv   \n",
    "python est_lands_csv.py data/spec_301_resample.h5  nn-heats --use-seg nn-segs --pat 1 --out spec_resample_301_lands_930.csv   \n",
    "python est_lands_csv.py data/spec_301_pad.h5  nn-heats --use-seg nn-segs --pat 1 --out spec_pad_301_lands_930.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "if USE_MY_DETE:\n",
    "    src_nn_csv_path = \"spec_301_lands_930.csv\"\n",
    "    ds_factor = float(0.2)\n",
    "    dst_fcsv_path = \"proj_301_lands.fcsv\"\n",
    "elif USE_PAD_DATA:\n",
    "    src_nn_csv_path = \"spec_pad_301_lands_930.csv\"\n",
    "    ds_factor = float(0.125)\n",
    "    dst_fcsv_path = \"proj_301_pad_lands.fcsv\"\n",
    "\n",
    "elif USE_RESAMPLE_DATA:\n",
    "    src_nn_csv_path = \"spec_resample_301_lands_930.csv\"\n",
    "    ds_factor = float(0.125)\n",
    "    dst_fcsv_path = \"proj_301_resample_lands.fcsv\"\n",
    "else:   \n",
    "    src_nn_csv_path = \"spec_1-6_lands.csv\"\n",
    "    ds_factor = float(0.125)\n",
    "    dst_fcsv_path = \"proj_x8_lands.fcsv\"\n",
    "pat_idx = 1\n",
    "proj_idx = 0\n",
    "\n",
    "write_lands_map_to_fcsv(\n",
    "    extract_lands_map_for_proj_from_nn_csv(src_nn_csv_path, pat_idx,\n",
    "                                            proj_idx, ds_factor),\n",
    "    dst_fcsv_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('monaienv': venv)"
  },
  "interpreter": {
   "hash": "41cc7d8094642f0aa3e27d256eb25d3485d313a694ac935cfb444781e1af4b99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}